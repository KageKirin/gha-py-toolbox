name: UTIL download files
description: >
  Downloads files.

inputs:
  urls:
    description: >
      URLs of the file to download
      Whitespace-separated values
    required: true
  directory:
    description: local download path for the downloaded files
    required: true
  path:
    description: local base path for the action
    required: false
    default: ${{github.workspace}}
  max-parallel:
    description: count of parallel downloads
    required: false
    default: 4


runs:
  using: composite
  steps:
  - name: Install dependencies
    uses: kagekirin/gha-py-toolbox/actions/pip/install@main
    with:
      packages: >-
        requests

  - id: download
    name: Download
    shell: python
    env:
      inputs_path: ${{inputs.path}}
      inputs_urls: ${{inputs.urls}}
      inputs_directory: ${{inputs.directory}}
      inputs_max_parallel: ${{inputs.max-parallel}}
    run: |
      ## actions/util/download-files-parallel/action.yml#download
      import os, shutil, requests
      import multiprocessing as mp
      from pathlib import Path
      from contextlib import chdir
      from urllib.parse import urlparse

      os.chdir(os.getenv("GITHUB_WOKRDIR", "."))

      inputs_max_parallel = os.getenv("inputs_max_parallel", "4")
      assert inputs_max_parallel is not None
      max_parallel = int(inputs_max_parallel)
      if max_parallel <= 0:
          max_parallel = mp.cpu_count()

      with chdir(os.getenv("inputs_path", ".")):
          inputs_urls = os.getenv("inputs_urls", "")
          assert inputs_urls is not None
          urls = str(inputs_urls).split()

          inputs_directory = os.getenv("inputs_directory", "download")
          assert inputs_directory is not None
          directory = Path(inputs_directory)
          directory.parent.mkdir(parents=True, exist_ok=True)

          def download(urlstr: str) -> requests.Response:
              url = urlparse(urlstr)
              with requests.get(url.geturl(), stream=True) as r:
                  if not r.ok:
                      print(
                          f"failed to access {url} with status {r.status_code} {r.reason}"
                      )
                  assert r.ok
                  if r.ok:
                      file = directory.join(Path(url.path).name)
                      with file.open("wb") as fh:
                          shutil.copyfileobj(r.raw, fh)
                  return r

          pool = mp.Pool(max_parallel)
          results = pool.map_async(download, urls).get()

          all_ok = all([r.ok for r in results])
          exit(0 if all_ok else 1)
